{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Classify tabular data into three categories (Control vs. ASD) using multiple scikit-learn models and feature selection with SelectKBest"
      ],
      "metadata": {
        "id": "AbJemsFCRbLf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The steps:\n",
        "<ol>\n",
        "<li><b>Load the Data: </b>Load your tabular data into a pandas DataFrame.</li>\n",
        "<li><b>Preprocess the Data:</b> Prepare the data for training, including handling missing values, encoding categorical variables, and splitting into features and labels.</li>\n",
        "<li><b>Feature Selection: </b> Use SelectKBest to select the top k features that are most relevant for classification.</li>\n",
        "<li><b>Model Training:</b> Train multiple scikit-learn models using the selected features.</li>\n",
        "<li><b>Model Evaluation:</b> Evaluate the models using cross-validation or a separate validation set and select the best-performing model.</li>\n",
        "<li><b>Model Saving:</b> Save the trained model for future use.</li></ol>"
      ],
      "metadata": {
        "id": "ER585OvKR9V1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "gZxpGdvFEVR9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lVYs2ueD0l3",
        "outputId": "c60a1c11-c04f-4b0a-ef7c-fc5b477347f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.neighbors import (NeighborhoodComponentsAnalysis,KNeighborsClassifier)\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from itertools import product\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.experimental import enable_hist_gradient_boosting\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "import pickle\n",
        "import os\n",
        "import xgboost as xgb\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.neighbors import RadiusNeighborsClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The data:\n",
        "Tabular data with the fields: <br>\n",
        "<ol>\n",
        "<li>\n",
        "<b>Type -</b> Control  / ASD ( another type of autism). </li>\n",
        "<li> <b>Num. of active electrodes -</b> (0 - 60) . </li>\n",
        "<li><b>Mean of all mean firing rate [spikes/sec] -</b> the average spike number during the investigation period (1 second) .  </li>\n",
        "<li><b>STD of mean firing rate -</b> the STD spike number during the investigation period (1 second)</li>\n",
        "<li><b>Mean of amplitudes  - </b> Mean of amplitudes.\n",
        "The amplitude of a periodic variable is a measure of its change in a single period (such as time). </li>\n",
        "<li><b>Std of amplitudes - </b>Std of amplitudes.\n",
        "\n"
      ],
      "metadata": {
        "id": "m7tvQ-3_EZDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.1 Making predictions on a custom image\n",
        "#It looks like our model does well qualitatively on data from the test set.\n",
        "#But how about on our own custom image?\n",
        "#That's where the real fun of machine learning is!\n",
        "#Predicting on your own custom data, outisde of any training or test set.\n",
        "\n",
        "# Download custom image\n",
        "import requests\n",
        "\n",
        "image_url = \"https://github.com/liatdavid2/my_custom_datasets/raw/main/Training_ASD_Table.csv\"\n",
        "image_name = image_url.split('/')[-1]\n",
        "# Setup custom image path\n",
        "data_path = Path(\"\")\n",
        "custom_image_path = data_path / image_name\n",
        "print(custom_image_path)\n",
        "\n",
        "# Download the image if it doesn't already exist\n",
        "if not custom_image_path.is_file():\n",
        "    with open(custom_image_path, \"wb\") as f:\n",
        "        # When downloading from GitHub, need to use the \"raw\" file link\n",
        "        request = requests.get(image_url)\n",
        "        print(f\"Downloading {custom_image_path}...\")\n",
        "        f.write(request.content)\n",
        "else:\n",
        "    print(f\"{custom_image_path} already exists, skipping download.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbPOTGy1PQZZ",
        "outputId": "c6402275-5964-44f3-e031-9fd737b594ef"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training_ASD_Table.csv\n",
            "Downloading Training_ASD_Table.csv...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate model accuracy,precision,recall,f1 and confusion matrix"
      ],
      "metadata": {
        "id": "ewkrhTMkMgo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Description: evaluate model accuracy,precision,recall,f1\n",
        "#              and confusion matrix\n",
        "# Input: y_test, y_pred and model_name:str\n",
        "# Output: f1:float\n",
        "def evaluate_model(y_test, y_pred,model_name):\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred,average='micro')\n",
        "    recall = recall_score(y_test, y_pred,average='micro')\n",
        "    f1 = f1_score(y_test, y_pred,average='micro')\n",
        "    false_positives = sum((y_test == 0) & (y_pred == 1))\n",
        "    false_negatives = sum((y_test == 1) & (y_pred == 0))\n",
        "\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "    \"\"\"print(\"False Positives:\", false_positives)\n",
        "    print(\"False Negatives:\", false_negatives)\n",
        "    cm = confusion_matrix(y_test,y_pred)\n",
        "    print(\"confusion matrix:\\n\",cm)\n",
        "    print(classification_report(y_test, y_pred, labels=[0, 1, 2]))\"\"\"\n",
        "    return f1"
      ],
      "metadata": {
        "id": "jgtz7qcqEYO2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train one of the model from models list with the data in the csv file and save him with his F1 score"
      ],
      "metadata": {
        "id": "ZhnDrGMTMy3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Description: build model from data csv file\n",
        "# Input: base_model and model_name:str\n",
        "def train_test_evaluate_model(model,model_name):\n",
        "    # Step 1: Read the Excel file into a DataFrame\n",
        "    file = \"Training_ASD_Table.csv\"  # Replace with the path to your csv file\n",
        "\n",
        "\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # Step 2: Preprocess the data if necessary\n",
        "    # For example, handle missing values or encode categorical variables\n",
        "    # encode categorical variables ID\n",
        "    label_encoder = LabelEncoder()\n",
        "    label_encoded_df = df.copy()\n",
        "    df['type'] = label_encoder.fit_transform(df['type'])\n",
        "\n",
        "\n",
        "    # Step 3: Extract features and labels\n",
        "    # Assuming the last column contains the label (control vs. patient)\n",
        "    X = df.drop(columns=['type'],axis=1).values\n",
        "    y = df['type'].values\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Step 4: Split the data into training and testing sets (80-20 split)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    # Step 5: Train with model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Step 6: Predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    #y_pred_proba = model.predict_proba(X_test)\n",
        "    #print(y_pred_proba)\n",
        "    print('\\n--------------'+model_name+' classifier: -----------------')\n",
        "    #for i in range(len(y_pred_proba)):\n",
        "    #    print('real: ',y_test[i],'pred: ',np.argmax(y_pred_proba[i]),'prob: ',max(y_pred_proba[i]))\n",
        "    # Step 7: Evaluate the model\n",
        "    f1 = evaluate_model(y_test, y_pred,model_name)\n",
        "    #print(label_encoder.classes_)\n",
        "    # save the model to disk\n",
        "    pickle.dump(model, open(\"models/F1_\"+str(round(f1, 2))+'_'+model_name+'file_'+file.split('.')[0], 'wb'))\n",
        "    print('Save model to '+\"models/F1_\"+str(round(f1, 2))+'_'+model_name+'file_'+file.split('.')[0])\n"
      ],
      "metadata": {
        "id": "rgUFS8R_M9FL"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model list to train"
      ],
      "metadata": {
        "id": "0RzyciQAORW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Description: train & evaluate model from data csv file\n",
        "def train_models_from_data():\n",
        "    if not os.path.exists(\"models\"):\n",
        "        os.makedirs(\"models\")\n",
        "    # Create and train the Random Forest classifier\n",
        "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    train_test_evaluate_model(rf_classifier,'Random_Forest')\n",
        "\n",
        "\n",
        "\n",
        "    # Create and train the Gradient Boosting classifier\n",
        "    gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0)\n",
        "    train_test_evaluate_model(gbc,'GradientBoosting')\n",
        "\n",
        "    # Create and train the Hist Gradient Boosting classifier\n",
        "    hgbc = HistGradientBoostingClassifier(max_iter=100)\n",
        "    train_test_evaluate_model(hgbc,'HistGradientBoosting_Classifier')\n",
        "\n",
        "    # Create and train the neural_network Multi-layer Perceptron classifier\n",
        "    MLP = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(15,), random_state=1)\n",
        "    train_test_evaluate_model(MLP,'Multi_layer_Perceptron_Classifier')\n",
        "    # Create and train Gaussian naive_bayes classifier\n",
        "    gnb = GaussianNB()\n",
        "    train_test_evaluate_model(gnb,'Gaussian_naive_bayes')\n",
        "\n",
        "    # Create and train SVM Kernel classifier\n",
        "    rbf_svc = SVC(kernel ='rbf', random_state = 0, probability=True)\n",
        "    train_test_evaluate_model(rbf_svc,'SVM_Kernel')\n",
        "\n",
        "    # Create and train the Gradient Voting classifier\n",
        "    eclf = VotingClassifier(estimators=[('rf_classifier', rf_classifier), ('hgbc', hgbc), ('gbc', gbc)],voting='soft', weights=[2, 1, 2])\n",
        "    train_test_evaluate_model(eclf,'Voting')\n",
        "\n",
        "    xgb_model  = xgb.XGBClassifier(n_jobs=1)\n",
        "    train_test_evaluate_model(xgb_model,'xgb_model')\n",
        "\n",
        "    # Create and train the KNeighbors classifier\n",
        "    nca = NeighborhoodComponentsAnalysis(random_state=42)\n",
        "    knn = KNeighborsClassifier(n_neighbors=3)\n",
        "    nca_pipe = Pipeline([('nca', nca), ('knn', knn)])\n",
        "    train_test_evaluate_model(nca_pipe,'KNeighbors')\n",
        "\n",
        "    #neigh = RadiusNeighborsClassifier(radius=2.5)\n",
        "    #train_test_evaluate_model(neigh,'neigh_radius_1_9')\n",
        "\n",
        "train_models_from_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfZY5KUyOLZM",
        "outputId": "b502860c-c9b7-42b8-fab4-58b942495917"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------Random_Forest classifier: -----------------\n",
            "Accuracy: 0.9090909090909091\n",
            "Precision: 0.9090909090909091\n",
            "Recall: 0.9090909090909091\n",
            "F1 Score: 0.9090909090909091\n",
            "Save model to models/F1_0.91_Random_Forestfile_Training_ASD_Table\n",
            "\n",
            "--------------GradientBoosting classifier: -----------------\n",
            "Accuracy: 0.7272727272727273\n",
            "Precision: 0.7272727272727273\n",
            "Recall: 0.7272727272727273\n",
            "F1 Score: 0.7272727272727273\n",
            "Save model to models/F1_0.73_GradientBoostingfile_Training_ASD_Table\n",
            "\n",
            "--------------HistGradientBoosting_Classifier classifier: -----------------\n",
            "Accuracy: 0.8181818181818182\n",
            "Precision: 0.8181818181818182\n",
            "Recall: 0.8181818181818182\n",
            "F1 Score: 0.8181818181818182\n",
            "Save model to models/F1_0.82_HistGradientBoosting_Classifierfile_Training_ASD_Table\n",
            "\n",
            "--------------Multi_layer_Perceptron_Classifier classifier: -----------------\n",
            "Accuracy: 0.9090909090909091\n",
            "Precision: 0.9090909090909091\n",
            "Recall: 0.9090909090909091\n",
            "F1 Score: 0.9090909090909091\n",
            "Save model to models/F1_0.91_Multi_layer_Perceptron_Classifierfile_Training_ASD_Table\n",
            "\n",
            "--------------Gaussian_naive_bayes classifier: -----------------\n",
            "Accuracy: 0.8181818181818182\n",
            "Precision: 0.8181818181818182\n",
            "Recall: 0.8181818181818182\n",
            "F1 Score: 0.8181818181818182\n",
            "Save model to models/F1_0.82_Gaussian_naive_bayesfile_Training_ASD_Table\n",
            "\n",
            "--------------SVM_Kernel classifier: -----------------\n",
            "Accuracy: 0.8181818181818182\n",
            "Precision: 0.8181818181818182\n",
            "Recall: 0.8181818181818182\n",
            "F1 Score: 0.8181818181818182\n",
            "Save model to models/F1_0.82_SVM_Kernelfile_Training_ASD_Table\n",
            "\n",
            "--------------Voting classifier: -----------------\n",
            "Accuracy: 0.9090909090909091\n",
            "Precision: 0.9090909090909091\n",
            "Recall: 0.9090909090909091\n",
            "F1 Score: 0.9090909090909091\n",
            "Save model to models/F1_0.91_Votingfile_Training_ASD_Table\n",
            "\n",
            "--------------xgb_model classifier: -----------------\n",
            "Accuracy: 0.7272727272727273\n",
            "Precision: 0.7272727272727273\n",
            "Recall: 0.7272727272727273\n",
            "F1 Score: 0.7272727272727273\n",
            "Save model to models/F1_0.73_xgb_modelfile_Training_ASD_Table\n",
            "\n",
            "--------------KNeighbors classifier: -----------------\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1 Score: 1.0\n",
            "Save model to models/F1_1.0_KNeighborsfile_Training_ASD_Table\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make predictions on new data in csv file and add column with prediction"
      ],
      "metadata": {
        "id": "qluA3adnYS2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input: model_path - the model path url\n",
        "#        csv_data_path - the data file\n",
        "# Description: make_predictions with model and save to csv\n",
        "def make_predictions(model_path,csv_data_path):\n",
        "    # Step 1: load the model from disk\n",
        "    loaded_model = pickle.load(open(model_path, 'rb'))\n",
        "    df = pd.read_csv(csv_data_path)\n",
        "\n",
        "    # Step 2: Preprocess the data if necessary\n",
        "    # For example, handle missing values or encode categorical variables\n",
        "    # encode categorical variables ID\n",
        "    label_encoder = LabelEncoder()\n",
        "    label_encoded_df = df.copy()\n",
        "    df['type_encoder'] = label_encoder.fit_transform(df['type'])\n",
        "\n",
        "    # Step 3: Extract features and labels\n",
        "    X = df.drop(columns=['type','type_encoder'],axis=1).values\n",
        "    #X = df.drop(columns=['type'],axis=1).values\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "\n",
        "    # Step 4: Predict labels\n",
        "    y_pred = loaded_model.predict(X)\n",
        "\n",
        "    # Step 5: save predicted labels to new column in csv\n",
        "    #print( label_encoder.inverse_transform(y_pred))\n",
        "    df['pred'] =  label_encoder.inverse_transform(y_pred)\n",
        "    df.to_csv(csv_data_path.split(\".\")[0]+'_with_pred.csv')\n",
        "    print('Save prediction to '+csv_data_path.split(\".\")[0]+'_with_pred.csv')\n",
        "\n",
        "\n",
        "\n",
        "best_model_path = '/content/models/F1_1.0_KNeighborsfile_Training_ASD_Table'\n",
        "csv_data_path = \"Training_ASD_Table.csv\"  # Replace with the path to your csv file\n",
        "make_predictions(best_model_path,csv_data_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__5YciBqYTa1",
        "outputId": "52e48115-f5ba-4f04-c4db-81b8c838c57b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Save prediction to Training_ASD_Table_with_pred.csv\n"
          ]
        }
      ]
    }
  ]
}